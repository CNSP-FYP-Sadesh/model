{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bcd765-510c-4c2b-8422-0c598b9b17e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy==1.21.6\n",
    "# !pip install pot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb7c3b94-5ef1-4cc6-9bc4-5c7a42505813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.spatial.distance import cdist, pdist, squareform, euclidean\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cd64c1-eb62-47ab-93f7-bc6d246c47db",
   "metadata": {},
   "source": [
    "Customize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439426b1-905c-45dd-b78f-c99133f8b792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# existing_csv_path = './Dataset/Dunnhumby Dataset/transaction_data.csv'\n",
    "\n",
    "# # Read the existing CSV file into a DataFrame\n",
    "# df = pd.read_csv(existing_csv_path)\n",
    "\n",
    "# column_mapping = {\n",
    "#     'household_key': 'user_id',\n",
    "#     'BASKET_ID': 'order_id',\n",
    "#     'PRODUCT_ID': 'product_id',\n",
    "#     'QUANTITY': 'quantity',\n",
    "# }\n",
    "\n",
    "# # Rename columns\n",
    "# df.rename(columns=column_mapping, inplace=True)\n",
    "# df.sort_values(by=['user_id', 'DAY'], inplace=True)\n",
    "# df['days_since_prior_order'] = df.groupby('user_id')['DAY'].diff()\n",
    "# df['days_since_prior_order'] = df['days_since_prior_order'].fillna(0).astype(int)\n",
    "# new_csv_path = './Dataset/Dunnhumby Dataset/updated_data.csv'\n",
    "\n",
    "# # Save the modified DataFrame to a new CSV file\n",
    "# df.to_csv(new_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318bd8b-acbf-4010-967c-0708c0dfc011",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = \"./Dataset/instacart-market-basket-analysis/order_products__train.csv\"\n",
    "path_prior = \"./Dataset/instacart-market-basket-analysis/order_products__prior.csv\"\n",
    "path_products = \"./Dataset/instacart-market-basket-analysis/products.csv\"\n",
    "\n",
    "train_orders = pd.read_csv(path_train)\n",
    "prior_orders = pd.read_csv(path_prior)\n",
    "products = pd.read_csv(path_products)\n",
    "\n",
    "#Turn the product ID to a string\n",
    "#This is necessary because Gensim's Word2Vec expects sentences, so we have to resort to this dirty workaround\n",
    "train_orders[\"product_id\"] = train_orders[\"product_id\"].astype(str)\n",
    "prior_orders[\"product_id\"] = prior_orders[\"product_id\"].astype(str)\n",
    "\n",
    "train_products = train_orders.groupby(\"order_id\").apply(lambda order: order['product_id'].tolist())\n",
    "prior_products = prior_orders.groupby(\"order_id\").apply(lambda order: order['product_id'].tolist())\n",
    "\n",
    "#Create the final sentences\n",
    "sentences = prior_products._append(train_products).values\n",
    "\n",
    "#Train Word2Vec model\n",
    "model = gensim.models.Word2Vec(sentences, vector_size=50, window=5, min_count=50, workers=4)\n",
    "\n",
    "model.save(\"./product2vec.model\")\n",
    "model.wv.save_word2vec_format(\"./product2vec.model.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0982566-56dd-4e4a-9c26-a5926eefd2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasketConstructor(object):\n",
    "    '''\n",
    "        Group products into baskets(type: list)\n",
    "    '''\n",
    "    def __init__(self, raw_data_dir, cache_dir):\n",
    "        self.raw_data_dir = raw_data_dir\n",
    "        self.cache_dir = cache_dir\n",
    "    \n",
    "    def get_orders(self):\n",
    "        '''\n",
    "            get order context information\n",
    "        '''\n",
    "        orders = pd.read_csv(self.raw_data_dir + 'orders.csv')[:50000]\n",
    "        orders = orders.fillna(0.0)\n",
    "        orders['days'] = orders.groupby(['user_id'])['days_since_prior_order'].cumsum()\n",
    "        orders['days_last'] = orders.groupby(['user_id'])['days'].transform(max)\n",
    "        orders['days_up_to_last'] = orders['days_last'] - orders['days']\n",
    "        del orders['days_last']\n",
    "        del orders['days']\n",
    "        return orders\n",
    "    \n",
    "    def get_orders_items(self, prior_or_train):\n",
    "        '''\n",
    "            get detailed information of prior or train orders \n",
    "        '''\n",
    "        orders_products = pd.read_csv(self.raw_data_dir + 'order_products__%s.csv'%prior_or_train)\n",
    "        return orders_products\n",
    "    \n",
    "    def get_users_orders(self, prior_or_train):\n",
    "        '''\n",
    "            get users' prior detailed orders\n",
    "        '''\n",
    "        if os.path.exists(self.cache_dir + 'users_orders.pkl'):\n",
    "            with open(self.cache_dir + 'users_orders.pkl', 'rb') as f:\n",
    "                users_orders = pickle.load(f)\n",
    "        else:\n",
    "            orders = self.get_orders()\n",
    "            order_products_prior = self.get_orders_items(prior_or_train)\n",
    "            users_orders = pd.merge(order_products_prior, orders[['user_id', 'order_id', 'order_number', 'days_up_to_last']], \n",
    "                        on = ['order_id'], how = 'left')\n",
    "            with open(self.cache_dir + 'users_orders.pkl', 'wb') as f:\n",
    "                pickle.dump(users_orders, f, pickle.HIGHEST_PROTOCOL)\n",
    "        return users_orders\n",
    "    \n",
    "    def get_users_products(self, prior_or_train):\n",
    "        '''\n",
    "            get users' all purchased products\n",
    "        '''\n",
    "        if os.path.exists(self.cache_dir + 'users_products.pkl'):\n",
    "            with open(self.cache_dir + 'users_products.pkl', 'rb') as f:\n",
    "                users_products = pickle.load(f)\n",
    "        else:\n",
    "            users_products = self.get_users_orders(prior_or_train)[['user_id', 'product_id']].drop_duplicates()\n",
    "            users_products['product_id'] = users_products.product_id.astype(int)\n",
    "            users_products['user_id'] = users_products.user_id.astype(int)\n",
    "            users_products = users_products.groupby(['user_id'])['product_id'].apply(list).reset_index()\n",
    "            with open(self.cache_dir + 'users_products.pkl', 'wb') as f:\n",
    "                pickle.dump(users_products, f, pickle.HIGHEST_PROTOCOL)\n",
    "        return users_products\n",
    "\n",
    "    def get_items(self, gran):\n",
    "        '''\n",
    "            get items' information\n",
    "            gran = [departments, aisles, products]\n",
    "        '''\n",
    "        items = pd.read_csv(self.raw_data_dir + '%s.csv'%gran)\n",
    "        return items\n",
    "    \n",
    "    def get_baskets(self, prior_or_train, reconstruct = False, none_idx = 49689):\n",
    "        '''\n",
    "            get users' baskets\n",
    "        '''\n",
    "        filepath = self.cache_dir + './basket_' + prior_or_train + '.pkl'\n",
    "       \n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'rb') as f:\n",
    "                up_basket = pickle.load(f)\n",
    "        else:          \n",
    "            up = self.get_users_orders(prior_or_train).sort_values(['user_id', 'order_number', 'product_id'], ascending = True)\n",
    "            uid_oid = up[['user_id', 'order_number']].drop_duplicates()\n",
    "            up = up[['user_id', 'order_number', 'product_id']]\n",
    "            up_basket = up.groupby(['user_id', 'order_number'])['product_id'].apply(list).reset_index()\n",
    "            up_basket = pd.merge(uid_oid, up_basket, on = ['user_id', 'order_number'], how = 'left')\n",
    "            for row in up_basket.loc[up_basket.product_id.isnull(), 'product_id'].index:\n",
    "                up_basket.at[row, 'product_id'] = [none_idx]\n",
    "            up_basket = up_basket.sort_values(['user_id', 'order_number'], ascending = True).groupby(['user_id'])['product_id'].apply(list).reset_index()\n",
    "            up_basket.columns = ['user_id', 'basket']\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(up_basket, f, pickle.HIGHEST_PROTOCOL)\n",
    "        return up_basket\n",
    "        \n",
    "    def get_item_history(self, prior_or_train, reconstruct = False, none_idx = 49689):\n",
    "        filepath = self.cache_dir + './item_history_' + prior_or_train + '.pkl'\n",
    "        if (not reconstruct) and os.path.exists(filepath):\n",
    "            with open(filepath, 'rb') as f:\n",
    "                item_history = pickle.load(f)\n",
    "        else:\n",
    "            up = self.get_users_orders(prior_or_train).sort_values(['user_id', 'order_number', 'product_id'], ascending = True)\n",
    "            item_history = up.groupby(['user_id', 'order_number'])['product_id'].apply(list).reset_index()\n",
    "            item_history.loc[item_history.order_number == 1, 'product_id'] = item_history.loc[item_history.order_number == 1, 'product_id'] + [none_idx]\n",
    "            item_history = item_history.sort_values(['user_id', 'order_number'], ascending = True)\n",
    "            # accumulate \n",
    "            item_history['product_id'] = item_history.groupby(['user_id'])['product_id'].transform(pd.Series.cumsum)\n",
    "            # get unique item list\n",
    "            item_history['product_id'] = item_history['product_id'].apply(set).apply(list)\n",
    "            item_history = item_history.sort_values(['user_id', 'order_number'], ascending = True)\n",
    "            # shift each group to make it history\n",
    "            item_history['product_id'] = item_history.groupby(['user_id'])['product_id'].shift(1)\n",
    "            for row in item_history.loc[item_history.product_id.isnull(), 'product_id'].index:\n",
    "                item_history.at[row, 'product_id'] = [none_idx]\n",
    "            item_history = item_history.sort_values(['user_id', 'order_number'], ascending = True).groupby(['user_id'])['product_id'].apply(list).reset_index()\n",
    "            item_history.columns = ['user_id', 'history_items']\n",
    "\n",
    "            with open(filepath, 'wb') as f:\n",
    "                pickle.dump(item_history, f, pickle.HIGHEST_PROTOCOL)\n",
    "        return item_history \n",
    "\n",
    "\n",
    "class TaFengBasketConstructor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def get_baskets(self):\n",
    "        with open('./data/TaFeng/user_tran', 'r') as fd:\n",
    "            lines = [l.strip().split()[2:-1] for l in fd.readlines()]\n",
    "        with open('./data/TaFeng/user_tran', 'r') as fd:\n",
    "            user_ids = [l.strip().split()[0] for l in fd.readlines()]\n",
    "        with open('./data/TaFeng/user_tran', 'r') as fd:\n",
    "            transaction_id = [l.strip().split()[1] for l in fd.readlines()]\n",
    "\n",
    "        df = pd.DataFrame()\n",
    "        df['User Id'] = user_ids\n",
    "        df['Transaction Id'] = transaction_id\n",
    "        df['Products'] = lines\n",
    "\n",
    "        df = df.groupby(['User Id', 'Transaction Id']).agg(sum).reset_index()\n",
    "        \n",
    "        all_baskets = []\n",
    "        for user in df['User Id']:\n",
    "            all_baskets.append([])\n",
    "            df_tmp = df[df['User Id'] == user]\n",
    "            for trans in df_tmp['Transaction Id']:\n",
    "                all_baskets[-1].append(df_tmp[df_tmp['Transaction Id'] == trans]['Products'].values[0])\n",
    "        \n",
    "        return all_baskets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d114306-60d0-48b6-b8f4-1cb170606cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWrapper(object):\n",
    "    def __init__(self, type):\n",
    "        if type == 'product':\n",
    "            # self.model = Word2Vec.load(\"product2vec_min_count_50.model\")\n",
    "            self.model = Word2Vec.load(\"product2vec.model\")\n",
    "            \n",
    "        elif type == 'aisles':\n",
    "            self.model = Word2Vec.load(\"aisles2vec_min_count_50.model\")\n",
    "            self.products = pd.read_csv(\"./Dataset/instacart-market-basket-analysis/products.csv\")\n",
    "            self.p2aisles = dict(zip(self.products.product_id.astype(str), self.products.aisle_id.astype(str)))\n",
    "            \n",
    "        elif type == 'tafeng_products':\n",
    "            self.model = Word2Vec.load(\"tafeng2vec_min_count_50.model\")\n",
    "            \n",
    "        self.vocab_len = len(self.model.wv)\n",
    "        self.word2index = dict(zip([self.model.wv.index_to_key[i] for i in range(self.vocab_len)],\n",
    "                              [i for i in range(self.vocab_len)]))\n",
    "        self.word_index_df = pd.DataFrame(data=list(self.word2index.items()), columns=['product_id', 'emb_id'])\n",
    "        \n",
    "    def p2aisle_f(self, i):\n",
    "        return self.p2aisles[i]\n",
    "\n",
    "    def lookup_ind_f(self, i):\n",
    "        return self.word2index[i]\n",
    "\n",
    "    def get_closest_of_set(self, item_id, set_of_candidates):\n",
    "        vec_of_interest = self.model.wv.vectors[item_id]\n",
    "        closest = np.argmin([euclidean(vec_of_interest, self.model.wv.vectors[x]) for x in set_of_candidates])\n",
    "        return set_of_candidates[closest]\n",
    "    \n",
    "    def find_closest_from_preds(self, pred, candidates_l_l):\n",
    "        closest_from_history = []\n",
    "        for p in pred:\n",
    "            closest_from_history.append(self.get_closest_of_set(p, [x for seq in candidates_l_l for x in seq]))\n",
    "        return closest_from_history\n",
    "        \n",
    "    def basket_dist_REMD(self, baskets):\n",
    "        #Relaxed EMD as lower bound. It is basically a nearest neighborhood search to \n",
    "        #find the closest word in doc B for each word in doc A and then take sum of all minimum distances.    \n",
    "        basket1_vecs = self.model.wv.vectors[[x for x in baskets[0]]]\n",
    "        basket2_vecs = self.model.wv.vectors[[x for x in baskets[1]]]\n",
    "        \n",
    "        distance_matrix = cdist(basket1_vecs, basket2_vecs)\n",
    "        \n",
    "        return max(np.mean(np.min(distance_matrix, axis=0)),\n",
    "                   np.mean(np.min(distance_matrix, axis=1)))\n",
    "        \n",
    "    def basket_dist_EMD(self, baskets):\n",
    "        basket1 = baskets[0]\n",
    "        basket2 = baskets[1]\n",
    "        dictionary = np.unique(list(basket1) + list(basket2))\n",
    "        vocab_len_ = len(dictionary)\n",
    "        product2ind = dict(zip(dictionary, np.arange(vocab_len_)))\n",
    "\n",
    "        # Compute distance matrix.\n",
    "        dictionary_vecs = self.model.wv.vectors[[x for x in dictionary]]\n",
    "        distance_matrix = squareform(pdist(dictionary_vecs))\n",
    "\n",
    "        if np.sum(distance_matrix) == 0.0:\n",
    "            # `emd` gets stuck if the distance matrix contains only zeros.\n",
    "            return float('inf')\n",
    "\n",
    "        def nbow(document):\n",
    "            bow = np.zeros(vocab_len_, dtype=np.float32)\n",
    "            for d in document:\n",
    "                bow[product2ind[d]] += 1.\n",
    "            return bow / len(document)\n",
    "\n",
    "        # Compute nBOW representation of documents.\n",
    "        d1 = nbow(basket1)\n",
    "        d2 = nbow(basket2)\n",
    "\n",
    "        # Compute WMD.\n",
    "        return ot.emd2(d1, d2, distance_matrix)\n",
    "        \n",
    "    def remove_products_wo_embeddings(self, all_baskets):\n",
    "        all_baskets_filtered = []\n",
    "        for s in all_baskets:\n",
    "            s_cp = []\n",
    "            for b in s:\n",
    "                b_cp = [x for x in b if x in self.model.wv.index_to_key]\n",
    "                if len(b_cp) > 0:\n",
    "                    s_cp.append(b_cp)\n",
    "            if len(s_cp) > 0:\n",
    "                all_baskets_filtered.append(s_cp)\n",
    "        return all_baskets_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f21a12-3640-4509-9f79-8ea231f6aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_change(item, func):\n",
    "    if isinstance(item, list):\n",
    "        return [nested_change(x, func) for x in item]\n",
    "    return func(item)\n",
    "\n",
    "\n",
    "def remove_products_which_are_uncommon(all_baskets, max_num=500):\n",
    "    print('Removing all but {} most common products'.format(max_num))\n",
    "    p = []\n",
    "    for s in all_baskets:\n",
    "        for b in s:\n",
    "            p.extend(b)\n",
    "    product_counter = Counter(p)\n",
    "    most_common_products = [x for x, _ in product_counter.most_common(max_num)]\n",
    "    all_baskets_filtered = []\n",
    "    for s in all_baskets:\n",
    "        s_cp = []\n",
    "        for b in s:\n",
    "            b_cp = [x for x in b if x in most_common_products]\n",
    "            if len(b_cp) > 0:\n",
    "                s_cp.append(b_cp)\n",
    "        if len(s_cp) > 0:\n",
    "            all_baskets_filtered.append(s_cp)\n",
    "    return all_baskets_filtered\n",
    "\n",
    "\n",
    "def remove_short_baskets(all_baskets, l_b = 5, l_s = 10):\n",
    "    all_baskets_filtered = []\n",
    "    for s in all_baskets:\n",
    "        s_cp = []\n",
    "        for b in s:\n",
    "            if len(b) > l_b:\n",
    "                s_cp.append(b)\n",
    "        if len(s_cp) > l_s:\n",
    "            all_baskets_filtered.append(s_cp)\n",
    "    return all_baskets_filtered\n",
    "\n",
    "\n",
    "def split_data(all_baskets):\n",
    "    train_ub, test_ub = train_test_split(all_baskets, test_size=0.05, random_state=0)\n",
    "    train_ub, val_ub = train_test_split(train_ub, test_size=0.05, random_state=0)\n",
    "    \n",
    "    test_ub_input = [x[:-1] for x in test_ub]\n",
    "    test_ub_target = [x[-1] for x in test_ub]\n",
    "    \n",
    "    val_ub_input = [x[:-1] for x in val_ub]\n",
    "    val_ub_target = [x[-1] for x in val_ub]\n",
    "    \n",
    "    return train_ub, val_ub_input, val_ub_target, test_ub_input, test_ub_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb19a9cb-f770-43e9-aa54-5f508b83baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnnDtw(object):    \n",
    "    def __init__(self, n_neighbors):\n",
    "        self.n_neighbors = n_neighbors  \n",
    "        self.length_to_consider = 10\n",
    "    \n",
    "    def _spring_dtw_distance(self, ts_a, ts_b, best_for_ts_a, d, d_lower_bound):\n",
    "        \"\"\"Returns the DTW subsequence similarity distance between two 2-D\n",
    "        timeseries numpy arrays.\n",
    "        \n",
    "        Following Subsequence Matching in Data Streams, Machiko Toyoda, Yasushi Sakurai\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        ts_a, ts_b : array of shape [n_samples, n_timepoints]\n",
    "            Two arrays containing n_samples of timeseries data\n",
    "            whose DTW distance between each sample of A and B\n",
    "            will be compared\n",
    "            \n",
    "        best_for_ts_a: list of length n_neighbors. The entries denote the\n",
    "            stortest distances found so far. This is for stopping the \n",
    "            calculation early utilizing a lower bound approximation.\n",
    "        \n",
    "        d : DistanceMetric object the distance measure used for market baskets A_i - B_j \n",
    "        in the DTW dynamic programming function\n",
    "        \n",
    "        d_lower_bound : Lower bound of DistanceMetric object the distance measure used for market baskets A_i - B_j \n",
    "        in the DTW dynamic programming function\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        DTW distance between A and B\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create cost matrix via broadcasting with large int\n",
    "        M, N = len(ts_a), len(ts_b)\n",
    "\n",
    "        #Compute REMD distances\n",
    "        REMD_gen = map(d_lower_bound, [(i,j) for i in ts_a for j in ts_b])\n",
    "        d_REMD_min = np.fromiter(REMD_gen, dtype=np.float)\n",
    "\n",
    "        #Break here if there is no chance that this is the shortest\n",
    "        if np.sum(d_REMD_min[np.argpartition(d_REMD_min, M)][:M]) > max(best_for_ts_a):\n",
    "            return np.inf, ts_b[0]\n",
    "\n",
    "        cost = np.inf * np.ones((M, N))\n",
    "\n",
    "        #Compute all distances\n",
    "        d_mat = np.zeros((M,N))\n",
    "        for i in range(M):\n",
    "            for j in range(N):\n",
    "                d_mat[i,j] = d((ts_a[i], ts_b[j]))\n",
    "\n",
    "        # Initialize the first row and column\n",
    "        cost[0, 0] = d((ts_a[0], ts_b[0]))\n",
    "        for i in range(1, M):\n",
    "            cost[i, 0] = cost[i-1, 0] + d_mat[i, 0]\n",
    "\n",
    "        for j in range(1, N):\n",
    "            cost[0, j] = d_mat[0, j]\n",
    "            \n",
    "        # Populate rest of cost matrix within window\n",
    "        for i in range(1, M):\n",
    "            w = 1.\n",
    "            for j in range(1, N):\n",
    "                choices = cost[i-1, j-1], cost[i, j-1], cost[i-1, j]\n",
    "                cost[i, j] = min(choices) + w * d_mat[i,j]\n",
    "\n",
    "        min_idx = np.argmin(cost[-1,:-1])\n",
    "        # Return DTW distance, prediction for next basket\n",
    "        return cost[-1,min_idx], ts_b[min_idx + 1]\n",
    "  \n",
    "    def _dist_matrix(self, x, y, d, d_lower_bound):\n",
    "        # Compute full distance matrix of dtw distnces between x and y\n",
    "        # x_s = np.shape(x)\n",
    "        # y_s = np.shape(y)\n",
    "        x_s = [len(x)]\n",
    "        y_s = [len(y)]\n",
    "        dm = np.inf * np.ones((x_s[0], y_s[0])) \n",
    "        next_baskets = np.empty((x_s[0], y_s[0]), dtype=object)\n",
    "        \n",
    "        for i in tqdm(range(0, x_s[0])):\n",
    "            # Ensure all elements of x have the same length\n",
    "            max_length = max(len(seq) for seq in x[i])\n",
    "            x[i] = [np.array(seq)[-max_length:] for seq in x[i]]\n",
    "\n",
    "            best_dist = [np.inf] * max(self.n_neighbors)\n",
    "            for j in range(0, y_s[0]):\n",
    "                # Ensure all elements of y have the same length\n",
    "                max_length_y = max(len(seq_y) for seq_y in y[j])\n",
    "                y[j] = [np.array(seq_y)[-max_length_y:] for seq_y in y[j]]\n",
    "\n",
    "                dist, pred = self._spring_dtw_distance(x[i], y[j], best_dist, d, d_lower_bound)\n",
    "                if dist < np.max(best_dist):\n",
    "                    best_dist[np.argmax(best_dist)] = dist               \n",
    "                dm[i, j] = dist\n",
    "                next_baskets[i, j] = pred\n",
    "    \n",
    "        return dm, next_baskets\n",
    "        \n",
    "    def predict(self, tr_d, te_d, d, d_lower_bound):\n",
    "        dm, predictions = self._dist_matrix(te_d, tr_d, d, d_lower_bound)\n",
    "        \n",
    "        preds_total_l = []\n",
    "        distances_total_l = []\n",
    "        for k in self.n_neighbors:\n",
    "            # Identify the k nearest neighbors\n",
    "            knn_idx = dm.argsort()[:, :k]\n",
    "            preds_k_l = []\n",
    "            distances_k_l = []\n",
    "                \n",
    "            for i in range(len(te_d)):\n",
    "                preds = [predictions[i][knn_idx[i][x]] for x in range(knn_idx.shape[1])]\n",
    "                distances = np.mean([dm[i][knn_idx[i][x]] for x in range(knn_idx.shape[1])])\n",
    "                pred_len = int(np.mean([len(te_d[i][x]) for x in range(len(te_d[i]))]))\n",
    "                preds = [x for x, y in Counter([n for s in preds for n in s]).most_common(pred_len)]                \n",
    "                preds_k_l.append(preds)\n",
    "                distances_k_l.append(distances)\n",
    "            preds_total_l.append(preds_k_l)\n",
    "            distances_total_l.append(distances_k_l)\n",
    "            \n",
    "        return preds_total_l, distances_total_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c129b4-ee0b-48c7-ada5-b4c0697bff87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run():\n",
    "    embedding_wrapper = EmbeddingWrapper('product')\n",
    "    bc = BasketConstructor('./Dataset/instacart-market-basket-analysis/', './Dataset/instacart-market-basket-analysis/')\n",
    "    ub_basket = bc.get_baskets('prior', reconstruct=False)\n",
    "\n",
    "    all_baskets = ub_basket.basket.values\n",
    "    all_baskets = nested_change(list(all_baskets), str)\n",
    "\n",
    "    all_baskets = embedding_wrapper.remove_products_wo_embeddings(all_baskets)\n",
    "    all_baskets = remove_products_which_are_uncommon(all_baskets)\n",
    "    all_baskets = remove_short_baskets(all_baskets)\n",
    "    all_baskets = nested_change(all_baskets, embedding_wrapper.lookup_ind_f)\n",
    "\n",
    "    train_ub, val_ub_input, val_ub_target, test_ub_input, test_ub_target = split_data(all_baskets)\n",
    "\n",
    "    knndtw = KnnDtw(n_neighbors=[5])\n",
    "    preds_all, distances = knndtw.predict(train_ub, val_ub_input, embedding_wrapper.basket_dist_EMD, \n",
    "                                          embedding_wrapper.basket_dist_REMD)\n",
    "    return preds_all, distances\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c470b6c-9613-48d1-9708-512473dfd924",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ac3e1-b8cb-46c7-8736-57aeae6d72d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
