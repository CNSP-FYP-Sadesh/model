{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7487,"sourceType":"datasetVersion","datasetId":4931},{"sourceId":782411,"sourceType":"datasetVersion","datasetId":408408}],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-15T11:15:22.887498Z","iopub.execute_input":"2024-01-15T11:15:22.887823Z","iopub.status.idle":"2024-01-15T11:15:23.890745Z","shell.execute_reply.started":"2024-01-15T11:15:22.887798Z","shell.execute_reply":"2024-01-15T11:15:23.889778Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/dunnhumby-the-complete-journey/campaign_table.csv\n/kaggle/input/dunnhumby-the-complete-journey/causal_data.csv\n/kaggle/input/dunnhumby-the-complete-journey/coupon.csv\n/kaggle/input/dunnhumby-the-complete-journey/campaign_desc.csv\n/kaggle/input/dunnhumby-the-complete-journey/product.csv\n/kaggle/input/dunnhumby-the-complete-journey/transaction_data.csv\n/kaggle/input/dunnhumby-the-complete-journey/hh_demographic.csv\n/kaggle/input/dunnhumby-the-complete-journey/coupon_redempt.csv\n/kaggle/input/instacart-market-basket-analysis/products.csv\n/kaggle/input/instacart-market-basket-analysis/order_products__train.csv\n/kaggle/input/instacart-market-basket-analysis/orders.csv\n/kaggle/input/instacart-market-basket-analysis/order_products__prior.csv\n/kaggle/input/instacart-market-basket-analysis/aisles.csv\n/kaggle/input/instacart-market-basket-analysis/departments.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pot","metadata":{"execution":{"iopub.status.busy":"2024-01-15T11:15:23.892703Z","iopub.execute_input":"2024-01-15T11:15:23.893101Z","iopub.status.idle":"2024-01-15T11:15:38.111570Z","shell.execute_reply.started":"2024-01-15T11:15:23.893073Z","shell.execute_reply":"2024-01-15T11:15:38.110397Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting pot\n  Obtaining dependency information for pot from https://files.pythonhosted.org/packages/71/9f/ea12f9cec44bf5101cfe34905875adbfc0d665ec6630dc7f0e320f249ef2/POT-0.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Downloading POT-0.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\nRequirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.10/site-packages (from pot) (1.24.3)\nRequirement already satisfied: scipy>=1.6 in /opt/conda/lib/python3.10/site-packages (from pot) (1.11.4)\nDownloading POT-0.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.0/823.0 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: pot\nSuccessfully installed pot-0.9.3\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport gensim\n\npath_train = \"/kaggle/input/instacart-market-basket-analysis/order_products__train.csv\"\npath_prior = \"/kaggle/input/instacart-market-basket-analysis/order_products__prior.csv\"\npath_products = \"/kaggle/input/instacart-market-basket-analysis/products.csv\"\n\ntrain_orders = pd.read_csv(path_train)\nprior_orders = pd.read_csv(path_prior)\nproducts = pd.read_csv(path_products)\n\n#Turn the product ID to a string\n#This is necessary because Gensim's Word2Vec expects sentences, so we have to resort to this dirty workaround\ntrain_orders[\"product_id\"] = train_orders[\"product_id\"].astype(str)\nprior_orders[\"product_id\"] = prior_orders[\"product_id\"].astype(str)\n\ntrain_products = train_orders.groupby(\"order_id\").apply(lambda order: order['product_id'].tolist())\nprior_products = prior_orders.groupby(\"order_id\").apply(lambda order: order['product_id'].tolist())\n\n#Create the final sentences\n","metadata":{"execution":{"iopub.status.busy":"2024-01-15T11:15:38.113033Z","iopub.execute_input":"2024-01-15T11:15:38.113353Z","iopub.status.idle":"2024-01-15T11:19:46.985170Z","shell.execute_reply.started":"2024-01-15T11:15:38.113309Z","shell.execute_reply":"2024-01-15T11:19:46.984319Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"}]},{"cell_type":"code","source":"prior_products","metadata":{"execution":{"iopub.status.busy":"2024-01-15T11:19:46.987719Z","iopub.execute_input":"2024-01-15T11:19:46.988487Z","iopub.status.idle":"2024-01-15T11:19:46.999145Z","shell.execute_reply.started":"2024-01-15T11:19:46.988450Z","shell.execute_reply":"2024-01-15T11:19:46.998037Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"order_id\n2          [33120, 28985, 9327, 45918, 30035, 17794, 4014...\n3          [33754, 24838, 17704, 21903, 17668, 46667, 174...\n4          [46842, 26434, 39758, 27761, 10054, 21351, 225...\n5          [13176, 15005, 47329, 27966, 23909, 48370, 132...\n6                                      [40462, 15873, 41897]\n                                 ...                        \n3421079                                              [30136]\n3421080    [27845, 4932, 18811, 41950, 31717, 12935, 2512...\n3421081     [38185, 12218, 32299, 3060, 20539, 35221, 12861]\n3421082    [17279, 12738, 16797, 43352, 32700, 12023, 47941]\n3421083    [7854, 45309, 21162, 18176, 35211, 39678, 1135...\nLength: 3214874, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"sentences = prior_products._append(train_products).values\n\n#Train Word2Vec model\nmodel = gensim.models.Word2Vec(sentences, vector_size=50, window=5, min_count=50, workers=4)\n\nmodel.save(\"product2vec.model\")\nmodel.wv.save_word2vec_format(\"product2vec.model.bin\", binary=True)","metadata":{"execution":{"iopub.status.busy":"2024-01-15T11:19:47.000445Z","iopub.execute_input":"2024-01-15T11:19:47.000784Z","iopub.status.idle":"2024-01-15T11:22:44.122843Z","shell.execute_reply.started":"2024-01-15T11:19:47.000753Z","shell.execute_reply":"2024-01-15T11:22:44.122037Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom collections import Counter\n\n\ndef nested_change(item, func):\n    if isinstance(item, list):\n        return [nested_change(x, func) for x in item]\n    return func(item)\n\n\ndef remove_products_which_are_uncommon(all_baskets, max_num=500):\n    print('Removing all but {} most common products'.format(max_num))\n    p = []\n    for s in all_baskets:\n        for b in s:\n            p.extend(b)\n    product_counter = Counter(p)\n    most_common_products = [x for x, _ in product_counter.most_common(max_num)]\n    all_baskets_filtered = []\n    for s in all_baskets:\n        s_cp = []\n        for b in s:\n            b_cp = [x for x in b if x in most_common_products]\n            if len(b_cp) > 0:\n                s_cp.append(b_cp)\n        if len(s_cp) > 0:\n            all_baskets_filtered.append(s_cp)\n    return all_baskets_filtered\n\n\ndef remove_short_baskets(all_baskets, l_b = 5, l_s = 10):\n    all_baskets_filtered = []\n    for s in all_baskets:\n        s_cp = []\n        for b in s:\n            if len(b) > l_b:\n                s_cp.append(b)\n        if len(s_cp) > l_s:\n            all_baskets_filtered.append(s_cp)\n    return all_baskets_filtered\n\n\ndef split_data(all_baskets):\n    train_ub, test_ub = train_test_split(all_baskets, test_size=0.05, random_state=0)\n    train_ub, val_ub = train_test_split(train_ub, test_size=0.05, random_state=0)\n    \n    test_ub_input = [x[:-1] for x in test_ub]\n    test_ub_target = [x[-1] for x in test_ub]\n    \n    val_ub_input = [x[:-1] for x in val_ub]\n    val_ub_target = [x[-1] for x in val_ub]\n    \n    return train_ub, val_ub_input, val_ub_target, test_ub_input, test_ub_target","metadata":{"execution":{"iopub.status.busy":"2024-01-15T11:22:44.124988Z","iopub.execute_input":"2024-01-15T11:22:44.125296Z","iopub.status.idle":"2024-01-15T11:22:44.284246Z","shell.execute_reply.started":"2024-01-15T11:22:44.125270Z","shell.execute_reply":"2024-01-15T11:22:44.283148Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom gensim.models import Word2Vec\nfrom scipy.spatial.distance import cdist, pdist, squareform, euclidean\nimport ot \n\n\nclass EmbeddingWrapper(object):\n    def __init__(self, type):\n        if type == 'product':\n            self.model = Word2Vec.load(\"/kaggle/working/product2vec.model\")\n            \n        elif type == 'aisles':\n            self.model = Word2Vec.load(\"aisles2vec_min_count_50.model\")\n            self.products = pd.read_csv(\"/kaggle/input/instacart-market-basket-analysis/products.csv\")\n            self.p2aisles = dict(zip(self.products.product_id.astype(str), self.products.aisle_id.astype(str)))\n            \n        elif type == 'tafeng_products':\n            self.model = Word2Vec.load(\"tafeng2vec_min_count_50.model\")\n        self.vocab_len = len(self.model.wv.index_to_key)\n        self.word2index = dict(zip([self.model.wv.index_to_key[i] for i in range(self.vocab_len)],\n                              [i for i in range(self.vocab_len)]))\n        self.word_index_df = pd.DataFrame(data=list(self.word2index.items()), columns=['product_id', 'emb_id'])\n        \n    def p2aisle_f(self, i):\n        return self.p2aisles[i]\n\n    def lookup_ind_f(self, i):\n        return self.word2index[i]\n\n    def get_closest_of_set(self, item_id, set_of_candidates):\n        vec_of_interest = self.model.wv.vectors[item_id]\n        closest = np.argmin([euclidean(vec_of_interest, self.model.wv.vectors[x]) for x in set_of_candidates])\n        return set_of_candidates[closest]\n    \n    def find_closest_from_preds(self, pred, candidates_l_l):\n        closest_from_history = []\n        for p in pred:\n            closest_from_history.append(self.get_closest_of_set(p, [x for seq in candidates_l_l for x in seq]))\n        return closest_from_history\n        \n    def basket_dist_REMD(self, baskets):\n        #Relaxed EMD as lower bound. It is basically a nearest neighborhood search to \n        #find the closest word in doc B for each word in doc A and then take sum of all minimum distances.    \n        basket1_vecs = self.model.wv.vectors[[x for x in baskets[0]]]\n        basket2_vecs = self.model.wv.vectors[[x for x in baskets[1]]]\n        \n        distance_matrix = cdist(basket1_vecs, basket2_vecs)\n        \n        return max(np.mean(np.min(distance_matrix, axis=0)),\n                   np.mean(np.min(distance_matrix, axis=1)))\n        \n    def basket_dist_EMD(self, baskets):\n        basket1 = baskets[0]\n        basket2 = baskets[1]\n        dictionary = np.unique(list(basket1) + list(basket2))\n        vocab_len_ = len(dictionary)\n        product2ind = dict(zip(dictionary, np.arange(vocab_len_)))\n\n        # Compute distance matrix.\n        dictionary_vecs = self.model.wv.vectors[[x for x in dictionary]]\n        distance_matrix = squareform(pdist(dictionary_vecs))\n\n        if np.sum(distance_matrix) == 0.0:\n            # `emd` gets stuck if the distance matrix contains only zeros.\n            return float('inf')\n\n        def nbow(document):\n            bow = np.zeros(vocab_len_, dtype=np.float)\n            for d in document:\n                bow[product2ind[d]] += 1.\n            return bow / len(document)\n\n        # Compute nBOW representation of documents.\n        d1 = nbow(basket1)\n        d2 = nbow(basket2)\n\n        # Compute WMD.\n        return ot.emd2(d1, d2, distance_matrix)\n        \n    def remove_products_wo_embeddings(self, all_baskets):\n        all_baskets_filtered = []\n        for s in all_baskets:\n            s_cp = []\n            for b in s:\n                b_cp = [x for x in b if x in self.model.wv.index_to_key]\n                if len(b_cp) > 0:\n                    s_cp.append(b_cp)\n            if len(s_cp) > 0:\n                all_baskets_filtered.append(s_cp)\n        return all_baskets_filtered","metadata":{"execution":{"iopub.status.busy":"2024-01-15T11:22:44.285965Z","iopub.execute_input":"2024-01-15T11:22:44.286300Z","iopub.status.idle":"2024-01-15T11:22:59.701413Z","shell.execute_reply.started":"2024-01-15T11:22:44.286271Z","shell.execute_reply":"2024-01-15T11:22:59.700573Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom collections import Counter\n# from embedding_wrapper import EmbeddingWrapper\nfrom tqdm import tqdm\n\nclass KnnDtw(object):    \n    def __init__(self, n_neighbors):\n        self.n_neighbors = n_neighbors  \n        self.length_to_consider = 10\n    \n    def _spring_dtw_distance(self, ts_a, ts_b, best_for_ts_a, d, d_lower_bound):\n        \"\"\"Returns the DTW subsequence similarity distance between two 2-D\n        timeseries numpy arrays.\n        \n        Following Subsequence Matching in Data Streams, Machiko Toyoda, Yasushi Sakurai\n\n        Arguments\n        ---------\n        ts_a, ts_b : array of shape [n_samples, n_timepoints]\n            Two arrays containing n_samples of timeseries data\n            whose DTW distance between each sample of A and B\n            will be compared\n            \n        best_for_ts_a: list of length n_neighbors. The entries denote the\n            stortest distances found so far. This is for stopping the \n            calculation early utilizing a lower bound approximation.\n        \n        d : DistanceMetric object the distance measure used for market baskets A_i - B_j \n        in the DTW dynamic programming function\n        \n        d_lower_bound : Lower bound of DistanceMetric object the distance measure used for market baskets A_i - B_j \n        in the DTW dynamic programming function\n        \n        Returns\n        -------\n        DTW distance between A and B\n        \"\"\"\n        \n        # Create cost matrix via broadcasting with large int\n        M, N = len(ts_a), len(ts_b)\n\n        #Compute REMD distances\n        REMD_gen = map(d_lower_bound, [(i,j) for i in ts_a for j in ts_b])\n        d_REMD_min = np.fromiter(REMD_gen, dtype=np.float)\n\n        #Break here if there is no chance that this is the shortest\n        if np.sum(d_REMD_min[np.argpartition(d_REMD_min, M)][:M]) > max(best_for_ts_a):\n            return np.inf, ts_b[0]\n\n        cost = np.inf * np.ones((M, N))\n\n        #Compute all distances\n        d_mat = np.zeros((M,N))\n        for i in range(M):\n            for j in range(N):\n                d_mat[i,j] = d((ts_a[i], ts_b[j]))\n\n        # Initialize the first row and column\n        cost[0, 0] = d((ts_a[0], ts_b[0]))\n        for i in range(1, M):\n            cost[i, 0] = cost[i-1, 0] + d_mat[i, 0]\n\n        for j in range(1, N):\n            cost[0, j] = d_mat[0, j]\n            \n        # Populate rest of cost matrix within window\n        for i in range(1, M):\n            w = 1.\n            for j in range(1, N):\n                choices = cost[i-1, j-1], cost[i, j-1], cost[i-1, j]\n                cost[i, j] = min(choices) + w * d_mat[i,j]\n\n        min_idx = np.argmin(cost[-1,:-1])\n        # Return DTW distance, prediction for next basket\n        return cost[-1,min_idx], ts_b[min_idx + 1]\n  \n    def _dist_matrix(self, x, y, d, d_lower_bound):\n        # Compute full distance matrix of dtw distnces between x and y\n        x_s = np.shape(x)\n        y_s = np.shape(y)\n        dm = np.inf * np.ones((x_s[0], y_s[0])) \n        next_baskets = np.empty((x_s[0], y_s[0]), dtype=object)\n        \n        for i in tqdm(range(0, x_s[0])):\n            x[i] = np.array(x[i])\n            if x[i].shape[0] > self.length_to_consider:\n                x[i] = x[i][-self.length_to_consider:]\n            best_dist = [np.inf] * max(self.n_neighbors)\n            for j in range(0, y_s[0]):\n                y[j] = np.array(y[j]) \n                dist, pred = self._spring_dtw_distance(x[i], y[j], best_dist, d, d_lower_bound)\n                if dist < np.max(best_dist):\n                    best_dist[np.argmax(best_dist)] = dist               \n                dm[i, j] = dist\n                next_baskets[i, j] = pred\n    \n        return dm, next_baskets\n        \n    def predict(self, tr_d, te_d, d, d_lower_bound):\n        dm, predictions = self._dist_matrix(te_d, tr_d, d, d_lower_bound)\n        \n        preds_total_l = []\n        distances_total_l = []\n        for k in self.n_neighbors:\n            # Identify the k nearest neighbors\n            knn_idx = dm.argsort()[:, :k]\n            preds_k_l = []\n            distances_k_l = []\n                \n            for i in range(len(te_d)):\n                preds = [predictions[i][knn_idx[i][x]] for x in range(knn_idx.shape[1])]\n                distances = np.mean([dm[i][knn_idx[i][x]] for x in range(knn_idx.shape[1])])\n                pred_len = int(np.mean([len(te_d[i][x]) for x in range(len(te_d[i]))]))\n                preds = [x for x, y in Counter([n for s in preds for n in s]).most_common(pred_len)]                \n                preds_k_l.append(preds)\n                distances_k_l.append(distances)\n            preds_total_l.append(preds_k_l)\n            distances_total_l.append(distances_k_l)\n            \n        return preds_total_l, distances_total_l","metadata":{"execution":{"iopub.status.busy":"2024-01-15T11:22:59.702863Z","iopub.execute_input":"2024-01-15T11:22:59.703639Z","iopub.status.idle":"2024-01-15T11:22:59.728669Z","shell.execute_reply.started":"2024-01-15T11:22:59.703602Z","shell.execute_reply":"2024-01-15T11:22:59.727699Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import os\nimport pickle\nimport pandas as pd\n    \n    \nclass BasketConstructor(object):\n    '''\n        Group products into baskets(type: list)\n    '''\n    def __init__(self, raw_data_dir, cache_dir):\n        self.raw_data_dir = raw_data_dir\n        self.cache_dir = cache_dir\n    \n    def get_orders(self):\n        '''\n            get order context information\n        '''\n        orders = pd.read_csv(self.raw_data_dir + 'orders.csv')\n        orders = orders.fillna(0.0)\n        orders['days'] = orders.groupby(['user_id'])['days_since_prior_order'].cumsum()\n        orders['days_last'] = orders.groupby(['user_id'])['days'].transform(max)\n        orders['days_up_to_last'] = orders['days_last'] - orders['days']\n        del orders['days_last']\n        del orders['days']\n        return orders\n    \n    def get_orders_items(self, prior_or_train):\n        '''\n            get detailed information of prior or train orders \n        '''\n        orders_products = pd.read_csv(self.raw_data_dir + 'order_products__%s.csv'%prior_or_train)\n        return orders_products\n    \n    def get_users_orders(self, prior_or_train):\n        '''\n            get users' prior detailed orders\n        '''\n        if os.path.exists(self.cache_dir + 'users_orders.pkl'):\n            with open(self.cache_dir + 'users_orders.pkl', 'rb') as f:\n                users_orders = pickle.load(f)\n        else:\n            orders = self.get_orders()\n            order_products_prior = self.get_orders_items(prior_or_train)\n            users_orders = pd.merge(order_products_prior, orders[['user_id', 'order_id', 'order_number', 'days_up_to_last']], \n                        on = ['order_id'], how = 'left')\n            with open(self.cache_dir + 'users_orders.pkl', 'wb') as f:\n                pickle.dump(users_orders, f, pickle.HIGHEST_PROTOCOL)\n        return users_orders\n    \n    def get_users_products(self, prior_or_train):\n        '''\n            get users' all purchased products\n        '''\n        if os.path.exists(self.cache_dir + 'users_products.pkl'):\n            with open(self.cache_dir + 'users_products.pkl', 'rb') as f:\n                users_products = pickle.load(f)\n        else:\n            users_products = self.get_users_orders(prior_or_train)[['user_id', 'product_id']].drop_duplicates()\n            users_products['product_id'] = users_products.product_id.astype(int)\n            users_products['user_id'] = users_products.user_id.astype(int)\n            users_products = users_products.groupby(['user_id'])['product_id'].apply(list).reset_index()\n            with open(self.cache_dir + 'users_products.pkl', 'wb') as f:\n                pickle.dump(users_products, f, pickle.HIGHEST_PROTOCOL)\n        return users_products\n\n    def get_items(self, gran):\n        '''\n            get items' information\n            gran = [departments, aisles, products]\n        '''\n        items = pd.read_csv(self.raw_data_dir + '%s.csv'%gran)\n        return items\n    \n    def get_baskets(self, prior_or_train, reconstruct = False, none_idx = 49689):\n        '''\n            get users' baskets\n        '''\n        filepath = self.cache_dir + './basket_' + prior_or_train + '.pkl'\n       \n        if os.path.exists(filepath):\n            with open(filepath, 'rb') as f:\n                up_basket = pickle.load(f)\n        else:          \n            up = self.get_users_orders(prior_or_train).sort_values(['user_id', 'order_number', 'product_id'], ascending = True)\n            uid_oid = up[['user_id', 'order_number']].drop_duplicates()\n            up = up[['user_id', 'order_number', 'product_id']]\n            up_basket = up.groupby(['user_id', 'order_number'])['product_id'].apply(list).reset_index()\n            up_basket = pd.merge(uid_oid, up_basket, on = ['user_id', 'order_number'], how = 'left')\n            for row in up_basket.loc[up_basket.product_id.isnull(), 'product_id'].index:\n                up_basket.at[row, 'product_id'] = [none_idx]\n            up_basket = up_basket.sort_values(['user_id', 'order_number'], ascending = True).groupby(['user_id'])['product_id'].apply(list).reset_index()\n            up_basket.columns = ['user_id', 'basket']\n            with open(filepath, 'wb') as f:\n                pickle.dump(up_basket, f, pickle.HIGHEST_PROTOCOL)\n        return up_basket\n        \n    def get_item_history(self, prior_or_train, reconstruct = False, none_idx = 49689):\n        filepath = self.cache_dir + './item_history_' + prior_or_train + '.pkl'\n        if (not reconstruct) and os.path.exists(filepath):\n            with open(filepath, 'rb') as f:\n                item_history = pickle.load(f)\n        else:\n            up = self.get_users_orders(prior_or_train).sort_values(['user_id', 'order_number', 'product_id'], ascending = True)\n            item_history = up.groupby(['user_id', 'order_number'])['product_id'].apply(list).reset_index()\n            item_history.loc[item_history.order_number == 1, 'product_id'] = item_history.loc[item_history.order_number == 1, 'product_id'] + [none_idx]\n            item_history = item_history.sort_values(['user_id', 'order_number'], ascending = True)\n            # accumulate \n            item_history['product_id'] = item_history.groupby(['user_id'])['product_id'].transform(pd.Series.cumsum)\n            # get unique item list\n            item_history['product_id'] = item_history['product_id'].apply(set).apply(list)\n            item_history = item_history.sort_values(['user_id', 'order_number'], ascending = True)\n            # shift each group to make it history\n            item_history['product_id'] = item_history.groupby(['user_id'])['product_id'].shift(1)\n            for row in item_history.loc[item_history.product_id.isnull(), 'product_id'].index:\n                item_history.at[row, 'product_id'] = [none_idx]\n            item_history = item_history.sort_values(['user_id', 'order_number'], ascending = True).groupby(['user_id'])['product_id'].apply(list).reset_index()\n            item_history.columns = ['user_id', 'history_items']\n\n            with open(filepath, 'wb') as f:\n                pickle.dump(item_history, f, pickle.HIGHEST_PROTOCOL)\n        return item_history \n\n\nclass TaFengBasketConstructor(object):\n    def __init__(self):\n        pass\n    \n    def get_baskets(self):\n        with open('./data/TaFeng/user_tran', 'r') as fd:\n            lines = [l.strip().split()[2:-1] for l in fd.readlines()]\n        with open('./data/TaFeng/user_tran', 'r') as fd:\n            user_ids = [l.strip().split()[0] for l in fd.readlines()]\n        with open('./data/TaFeng/user_tran', 'r') as fd:\n            transaction_id = [l.strip().split()[1] for l in fd.readlines()]\n\n        df = pd.DataFrame()\n        df['User Id'] = user_ids\n        df['Transaction Id'] = transaction_id\n        df['Products'] = lines\n\n        df = df.groupby(['User Id', 'Transaction Id']).agg(sum).reset_index()\n        \n        all_baskets = []\n        for user in df['User Id']:\n            all_baskets.append([])\n            df_tmp = df[df['User Id'] == user]\n            for trans in df_tmp['Transaction Id']:\n                all_baskets[-1].append(df_tmp[df_tmp['Transaction Id'] == trans]['Products'].values[0])\n        \n        return all_baskets","metadata":{"execution":{"iopub.status.busy":"2024-01-15T11:22:59.730188Z","iopub.execute_input":"2024-01-15T11:22:59.731100Z","iopub.status.idle":"2024-01-15T11:22:59.764922Z","shell.execute_reply.started":"2024-01-15T11:22:59.731056Z","shell.execute_reply":"2024-01-15T11:22:59.764105Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"\n\ndef run():\n    embedding_wrapper = EmbeddingWrapper('product')\n    print(\"Wrapped\")\n    bc = BasketConstructor('/kaggle/input/instacart-market-basket-analysis/', '/kaggle/working/')\n    print(\"Wrapped 2..\")\n\n    ub_basket = bc.get_baskets('prior', reconstruct=False)\n\n    all_baskets = ub_basket.basket.values\n    all_baskets = nested_change(list(all_baskets), str)\n    \n    all_baskets = embedding_wrapper.remove_products_wo_embeddings(all_baskets)\n    print(\"1\")\n\n    all_baskets = remove_products_which_are_uncommon(all_baskets)\n    print(\"2\")\n\n    all_baskets = remove_short_baskets(all_baskets)\n    print(\"3\")\n\n    all_baskets = nested_change(all_baskets, embedding_wrapper.lookup_ind_f)\n    print(\"4\")\n\n    train_ub, val_ub_input, val_ub_target, test_ub_input, test_ub_target = split_data(all_baskets)\n    print(\"Splitted..\")\n    knndtw = KnnDtw(n_neighbors=[5])\n    print(\"Predicting..\")\n\n    preds_all, distances = knndtw.predict(train_ub, val_ub_input, embedding_wrapper.basket_dist_EMD, \n                                          embedding_wrapper.basket_dist_REMD)\n    return preds_all, distances\n    \n\nif __name__ == \"__main__\":\n    run()","metadata":{"execution":{"iopub.status.busy":"2024-01-15T11:22:59.767553Z","iopub.execute_input":"2024-01-15T11:22:59.767955Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Wrapped\nWrapped 2..\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_27/3891660385.py:21: FutureWarning: The provided callable <built-in function max> is currently using SeriesGroupBy.max. In a future version of pandas, the provided callable will be used directly. To keep current behavior pass the string \"max\" instead.\n  orders['days_last'] = orders.groupby(['user_id'])['days'].transform(max)\n","output_type":"stream"}]}]}